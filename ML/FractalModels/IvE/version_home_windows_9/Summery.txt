Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 64, 64, 4)]       0         
                                                                 
 conv2d (Conv2D)             (None, 64, 64, 64)        320       
                                                                 
 dropout (Dropout)           (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        4160      
                                                                 
 dropout_1 (Dropout)         (None, 64, 64, 64)        0         
                                                                 
 flatten (Flatten)           (None, 262144)            0         
                                                                 
 dense (Dense)               (None, 2)                 524290    
                                                                 
=================================================================
Total params: 528,770
Trainable params: 528,770
Non-trainable params: 0
_________________________________________________________________

input_layer = tf.keras.Input(shape = (64, 64, 4))
x = tf.keras.layers.Conv2D(64, (1, 1), activation = "gelu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.01))(input_layer)
x = tf.keras.layers.Dropout(.25)(x)
x = tf.keras.layers.Conv2D(64, (1, 1), activation = "gelu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.01))(x)
x = tf.keras.layers.Dropout(.25)(x)
# x = tf.keras.layers.Conv2D(64, (1, 1), activation = "gelu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.01))(x)
# x = tf.keras.layers.Dropout(.25)(x)
x = tf.keras.layers.Flatten()(x)
output_layer = tf.keras.layers.Dense(output_classes, activation = "softmax")(x)

model = tf.keras.Model(inputs = input_layer, outputs = output_layer)

Still having an issue with validation accuracy staying flat. It might be the first layer, but it could also just be overtraining
in general.

Also of interest, and this makes sense and what I expected, binary cross entropy gives the same resutls for categorical cross entropy when the categories are 2
The model did not really improve or degrad with the change of functions.