Seed: 2117897

Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 64, 64, 4)]       0         
                                                                 
 conv2d (Conv2D)             (None, 64, 64, 64)        320       
                                                                 
 dropout (Dropout)           (None, 64, 64, 64)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 64, 64)        4160      
                                                                 
 dropout_1 (Dropout)         (None, 64, 64, 64)        0         
                                                                 
 flatten (Flatten)           (None, 262144)            0         
                                                                 
 dense (Dense)               (None, 2)                 524290    
                                                                 
=================================================================
Total params: 528,770
Trainable params: 528,770
Non-trainable params: 0
_________________________________________________________________


input_layer = tf.keras.Input(shape = (64, 64, 4))
x = tf.keras.layers.Conv2D(64, (1, 1), activation = "gelu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.001))(input_layer)
x = tf.keras.layers.Dropout(.35)(x)
x = tf.keras.layers.Conv2D(64, (1, 1), activation = "gelu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.001))(x)
x = tf.keras.layers.Dropout(.35)(x)
# x = tf.keras.layers.Conv2D(64, (1, 1), activation = "gelu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.01))(x)
# x = tf.keras.layers.Dropout(.25)(x)
x = tf.keras.layers.Flatten()(x)
output_layer = tf.keras.layers.Dense(output_classes, activation = "softmax")(x)

model = tf.keras.Model(inputs = input_layer, outputs = output_layer)


Dropouts seemed to hinder it. Maybe it wasn't run long enough (training could go higher) but the loss function seemed to go flat.