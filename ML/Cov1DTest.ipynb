{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pathlib\n",
    "import re\n",
    "# from tensorflow.keras.models import Sequential, load_module\n",
    "# from tensorflow.keras.layers import Conv1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import keras\n",
    "# https://stackoverflow.com/questions/53066762/understanding-1d-convolution-of-dna-sequences-encoded-as-a-one-hot-vector\n",
    "# https://stackoverflow.com/questions/53514495/what-does-batch-repeat-and-shuffle-do-with-tensorflow-dataset\n",
    "# https://stackoverflow.com/questions/53066762/understanding-1d-convolution-of-dna-sequences-encoded-as-a-one-hot-vector\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_class = 3\n",
    "# n_features = 100\n",
    "# n_sample = 1000\n",
    "\n",
    "# X = np.random.randint(0,10, (n_sample,n_features))\n",
    "# y = pd.get_dummies(np.random.randint(0,n_class, n_sample)).values\n",
    "\n",
    "# print(X)\n",
    "# print(X.shape)\n",
    "# # print(type(X))\n",
    "# print(X[0])\n",
    "\n",
    "# print(y)\n",
    "# print(y.shape)\n",
    "# print(type(y))\n",
    "\n",
    "# inp = tf.keras.Input((n_features,))\n",
    "# x = tf.keras.layers.Dense(128, activation='relu')(inp)\n",
    "# out = tf.keras.layers.Dense(n_class, activation='softmax')(x)\n",
    "\n",
    "# # model = tf.keras.Model(inp, out)\n",
    "# # model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# # history = model.fit(X, y, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_class = 3\n",
    "# n_features = 100\n",
    "# n_sample = 1000\n",
    "\n",
    "# X = np.random.randint(0,10, (n_sample,n_features))\n",
    "# y = np.random.randint(0,n_class, n_sample)\n",
    "\n",
    "# inp = tf.keras.Input((n_features,))\n",
    "# x = tf.keras.layers.Dense(128, activation='relu')(inp)\n",
    "# out = tf.keras.layers.Dense(n_class, activation='softmax')(x)\n",
    "\n",
    "# print(X)\n",
    "# print(X.shape)\n",
    "# print(type(X))\n",
    "\n",
    "# print(y)\n",
    "# print(y.shape)\n",
    "# print(type(y))\n",
    "\n",
    "# # model = tf.keras.Model(inp, out)\n",
    "# # model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# # history = model.fit(X, y, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# epochs = 50\n",
    "# steps_per_epoch = 10\n",
    "\n",
    "# https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
    "\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    '''\n",
    "    This is not really used, not sure why I'm keeping it, but it could be useful?\n",
    "    '''\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_load = np.load(cwd / \"Data.npy\")\n",
    "# train_lb_load = np.load(cwd / \"Labels.npy\")\n",
    "\n",
    "train_lb = np.load(cwd / \"Labels.npy\")\n",
    "keep = np.where(train_lb >= 1)[0]\n",
    "train_lb = train_lb[keep]\n",
    "train_lb = train_lb - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = train_ds_load\n",
    "# train_lb = train_lb_load\n",
    "\n",
    "# train_ds = save_train_ds\n",
    "# save_train_ds = train_ds\n",
    "# save_hold_lb = hold_lb\n",
    "\n",
    "# print(np.unique(hold_lb))\n",
    "# print(np.unique(hold_lb - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22956, 100000, 4)\n",
      "(22956, 1)\n",
      "[0 1 2 3]\n",
      "22956\n"
     ]
    }
   ],
   "source": [
    "# del train_ds\n",
    "# print(keep)\n",
    "# print(type(keep))\n",
    "train_ds = np.load(cwd / \"Data.npy\")\n",
    "# train_ds = np.ndarray(train_ds, dtype=\"int8\")\n",
    "train_ds = train_ds[keep]\n",
    "\n",
    "print(train_ds.shape)\n",
    "print(train_lb.shape)\n",
    "# print(np.unique(train_lb))\n",
    "\n",
    "\n",
    "# train_lb = np.ndarray(shape = (train_lb.shape[0]))\n",
    "# labels = np.ndarray(shape = (rows, 1))\n",
    "# for i, lb in enumerate(hold_lb):\n",
    "#     # if lb == 1:\n",
    "#     #     train_lb[i] = 1\n",
    "#     # if lb == 2:\n",
    "#     #     train_lb[i] = 2\n",
    "#     # if lb == 3:\n",
    "#     #     train_lb[i] = 3\n",
    "#     train_lb[i] = lb - 1\n",
    "print(np.unique(train_lb))\n",
    "\n",
    "all_samples = train_lb.shape[0]\n",
    "print(all_samples)\n",
    "\n",
    "\n",
    "# data_set = tf.data.Dataset.zip((train_ds, train_lb))\n",
    "\n",
    "# # print(train_lb)\n",
    "# # print(train_lb.shape)\n",
    "\n",
    "# valid_ds = train_ds[300:]\n",
    "# valid_lb = train_lb[300:]\n",
    "\n",
    "# index2delete = [i for i in range(300, 349)]\n",
    "\n",
    "# train_ds = np.delete(train_ds, index2delete, 0)\n",
    "# train_lb = np.delete(train_lb, index2delete, 0)\n",
    "\n",
    "# # input_shape = (train_ds.shape[0], train_ds.shape[1], train_ds.shape[2])\n",
    "# input_shape = train_ds.shape\n",
    "# # print(input_shape)\n",
    "# # print(train_lb)\n",
    "\n",
    "# print(train_ds.shape, train_lb.shape)\n",
    "# print(valid_ds.shape, valid_lb.shape)\n",
    "# # print(train_lb[0])\n",
    "# # print(type(train_lb))\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "# dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22956, 100000, 4)\n",
      "(22956,)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.shape)\n",
    "print(keep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2295, 100000, 4)\n",
      "(2295, 1)\n",
      "(2296, 100000, 4)\n",
      "(2296, 1)\n",
      "(18365, 100000, 4)\n",
      "(18365, 1)\n"
     ]
    }
   ],
   "source": [
    "random_sample = int(0.2 * all_samples)\n",
    "\n",
    "tv_samples = np.random.choice(all_samples, size = random_sample, replace=False)\n",
    "# print(tv_samples.shape)\n",
    "# print(np.unique(tv_samples).shape)\n",
    "# print(type(tv_samples))\n",
    "\n",
    "tv_sample = tv_samples.shape[0]\n",
    "\n",
    "test_sample = int(tv_sample / 2)\n",
    "valid_sample = tv_sample - test_sample\n",
    "# print(test_sample, valid_sample)\n",
    "\n",
    "test_samples = tv_samples[0:test_sample]\n",
    "# test_samples = set(test_samples)\n",
    "valid_samples = tv_samples[valid_sample - 1:tv_samples.shape[0]]\n",
    "# valid_samples = set(valid_samples)\n",
    "\n",
    "test_ds = train_ds[test_samples]\n",
    "print(test_ds.shape)\n",
    "test_lb = train_lb[test_samples]\n",
    "print(test_lb.shape)\n",
    "\n",
    "valid_ds = train_ds[valid_samples]\n",
    "print(valid_ds.shape)\n",
    "valid_lb = train_lb[valid_samples]\n",
    "print(valid_lb.shape)\n",
    "\n",
    "train_ds = np.delete(train_ds, tv_samples, axis = 0)\n",
    "print(train_ds.shape)\n",
    "train_lb = np.delete(train_lb, tv_samples, axis = 0)\n",
    "print(train_lb.shape)\n",
    "\n",
    "\n",
    "# print(valid_samples.intersection(test_samples))\n",
    "\n",
    "# print(test_samples.shape)\n",
    "# print(valid_samples.shape)\n",
    "\n",
    "\n",
    "# print(train_ds.shape)\n",
    "# print(hold_lb.shape)\n",
    "# print(np.unique(hold_lb_load))\n",
    "\n",
    "# keep = np.where(hold_lb >= 1)[0]\n",
    "# train_ds = train_ds[keep]\n",
    "# hold_lb = hold_lb[keep]\n",
    "# print(np.unique(hold_lb))\n",
    "\n",
    "# hold_lb = hold_lb - 1\n",
    "# print(np.unique(hold_lb))\n",
    "# print(train_ds.shape)\n",
    "# print(hold_lb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch = 368\n",
      "Meaning 18400 Seqs will be processed per Epoch\n",
      "For 18365 Seqs\n"
     ]
    }
   ],
   "source": [
    "data_size = train_ds.shape[0]\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 100\n",
    "steps_per_epoch = int(data_size / batch_size) + 1\n",
    "print(f\"Steps per epoch = {steps_per_epoch}\\nMeaning {batch_size * steps_per_epoch} Seqs will be processed per Epoch\\nFor {data_size} Seqs\")\n",
    "\n",
    "# https://stackoverflow.com/questions/64787511/tensorflow-dataset-batch-size-and-steps-per-epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_ds, train_lb))\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "dataset = dataset.batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = tf.data.Dataset.from_tensor_slices((test_ds, test_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape = (100000, 4))\n",
    "x = tf.keras.layers.Conv1D(4, 1)(input_layer)\n",
    "# x = tf.keras.layers.Dropout(.1)(x)\n",
    "# x = tf.keras.layers.AveragePooling1D(20)(x)\n",
    "x = tf.keras.layers.Conv1D(20, 1)(x)\n",
    "# x = tf.keras.layers.Conv1D(4, 1, strides = 20)(x)\n",
    "# x = tf.keras.layers.Conv1D(4, 1)(x)\n",
    "# x = tf.keras.layers.Conv1D(4, 1)(x)\n",
    "x = tf.keras.layers.Dropout(.5)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "output_layer = tf.keras.layers.Dense(4, activation = \"softmax\")(x)\n",
    "# output_layer = tf.keras.layers.Dense(3, activation = \"softmax\")(x)\n",
    "\n",
    "# # 2/28/2023 Attempt\n",
    "\n",
    "# input_layer = tf.keras.Input(shape = (100000, 4))\n",
    "# x = tf.keras.layers.Conv1D(4, 1)(input_layer)\n",
    "# x = tf.keras.layers.Conv1D(4, 1, strides = 20)(x)\n",
    "# # x = tf.keras.layers.Conv1D(4, 1)(x)\n",
    "# # x = tf.keras.layers.Conv1D(4, 1)(x)\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "# output_layer = tf.keras.layers.Dense(4, activation = \"softmax\")(x)\n",
    "# # output_layer = tf.keras.layers.Dense(3, activation = \"softmax\")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100000, 4)]       0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100000, 4)         20        \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 100000, 20)        100       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100000, 20)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2000000)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 8000004   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,000,124\n",
      "Trainable params: 8,000,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "            #   metrics = ['accuracy'])\n",
    "              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "368/368 [==============================] - 296s 802ms/step - loss: 0.9519 - sparse_categorical_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "368/368 [==============================] - 297s 807ms/step - loss: 0.6393 - sparse_categorical_accuracy: 0.7224\n",
      "Epoch 3/20\n",
      "368/368 [==============================] - 298s 810ms/step - loss: 0.5806 - sparse_categorical_accuracy: 0.7468\n",
      "Epoch 4/20\n",
      "368/368 [==============================] - 299s 812ms/step - loss: 0.5560 - sparse_categorical_accuracy: 0.7586\n",
      "Epoch 5/20\n",
      "368/368 [==============================] - 299s 812ms/step - loss: 0.5387 - sparse_categorical_accuracy: 0.7702\n",
      "Epoch 6/20\n",
      "368/368 [==============================] - 299s 811ms/step - loss: 0.5436 - sparse_categorical_accuracy: 0.7682\n",
      "Epoch 7/20\n",
      "368/368 [==============================] - 296s 803ms/step - loss: 0.5009 - sparse_categorical_accuracy: 0.7811\n",
      "Epoch 8/20\n",
      "368/368 [==============================] - 293s 797ms/step - loss: 0.5209 - sparse_categorical_accuracy: 0.7773\n",
      "Epoch 9/20\n",
      "368/368 [==============================] - 294s 798ms/step - loss: 0.5005 - sparse_categorical_accuracy: 0.7815\n",
      "Epoch 10/20\n",
      "368/368 [==============================] - 294s 800ms/step - loss: 0.5004 - sparse_categorical_accuracy: 0.7828\n",
      "Epoch 11/20\n",
      "368/368 [==============================] - 295s 801ms/step - loss: 0.4924 - sparse_categorical_accuracy: 0.7846\n",
      "Epoch 12/20\n",
      "368/368 [==============================] - 295s 801ms/step - loss: 0.4953 - sparse_categorical_accuracy: 0.7841\n",
      "Epoch 13/20\n",
      "368/368 [==============================] - 294s 799ms/step - loss: 0.5048 - sparse_categorical_accuracy: 0.7823\n",
      "Epoch 14/20\n",
      "368/368 [==============================] - 295s 802ms/step - loss: 0.4879 - sparse_categorical_accuracy: 0.7852\n",
      "Epoch 15/20\n",
      "368/368 [==============================] - 296s 803ms/step - loss: 0.4780 - sparse_categorical_accuracy: 0.7889\n",
      "Epoch 16/20\n",
      "368/368 [==============================] - 295s 803ms/step - loss: 0.4774 - sparse_categorical_accuracy: 0.7909\n",
      "Epoch 17/20\n",
      "368/368 [==============================] - 295s 803ms/step - loss: 0.5332 - sparse_categorical_accuracy: 0.7816\n",
      "Epoch 18/20\n",
      "368/368 [==============================] - 295s 802ms/step - loss: 0.4594 - sparse_categorical_accuracy: 0.7950\n",
      "Epoch 19/20\n",
      "368/368 [==============================] - 295s 803ms/step - loss: 0.4627 - sparse_categorical_accuracy: 0.7972\n",
      "Epoch 20/20\n",
      "368/368 [==============================] - 296s 803ms/step - loss: 0.4644 - sparse_categorical_accuracy: 0.7932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1edffdacd30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(train_ds, train_lb, batch_size=50, epochs=3, steps_per_epoch=10, validation_split=.2)\n",
    "model.fit(dataset, batch_size=batch_size, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# def eval_input_fn():\n",
    "#     batched_dataset = dataset.test(flags_obj.data_dir).batch(flags_obj.batch_size)\n",
    "#     return batched_dataset.__iter__()\n",
    "\n",
    "# File \"c:\\Users\\tokyo\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n",
    "        # target.shape.assert_is_compatible_with(output.shape)\n",
    "    # ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
    "\n",
    "# Is the shape of the cross entropy wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 6s 125ms/step - loss: 1.2304 - sparse_categorical_accuracy: 0.6780\n",
      "Results are: loss = 1.2303507328033447, accuracy = 0.6779956221580505\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_ds, test_lb, batch_size=batch_size)\n",
    "print(f\"Results are: loss = {results[0]}, accuracy = {results[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0099da87c5947ce286d3b9cb8658368d51c29569610ee4f8e225db5e1ddfd0e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
